{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#HomeWork #7\n",
        "\n",
        "#Theory Homework\n",
        "\n",
        "1.What are the key architectural features that make these systems suitable for AI workloads?\n",
        "\n",
        "2.Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models.\n",
        "\n",
        "3.Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g., SambaNova or Cerebras). What tools or software stacks are typically used in this process?\n",
        "\n",
        "4.Give an example of a project that would benefit from AI accelerators and why?"
      ],
      "metadata": {
        "id": "wV6OK9eK3gFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-Key Architectural Features:** AI accelerators like Cerebras and SambaNova are designed for AI workloads with features such as massive parallelism, high memory bandwidth, and low-latency communication. They focus on efficient data movement, scalability, and hardware-software integration to handle the computational intensity of tasks like deep learning model training and inference.\n",
        "\n",
        "**2-Primary Differences:** Cerebras employs a wafer-scale chip with hundreds of thousands of cores for sparse, large-scale neural networks, while SambaNova uses a reconfigurable dataflow architecture optimized for flexibility in AI workloads. Programming models also differ, with Cerebras leveraging its CSP and SambaNova supporting dataflow graphs, both of which are distinct from GPU programming with CUDA.\n",
        "\n",
        "**3-Workflow for Refactoring AI Models:** Refactoring involves selecting the AI model, adapting it to the constraints of the target system (e.g., splitting operations), and optimizing it with tools like SambaFlow or CSP. Testing and debugging follow, with deployment on the accelerator and iterative performance tuning to maximize efficiency.\n",
        "\n",
        "**4-Example Project:** Training large NLP models like GPT-4 benefits from AI accelerators due to their high memory bandwidth, parallelism, and scalability. These systems reduce training time and costs by efficiently handling large datasets and model sizes while supporting advanced optimizations for AI-specific workloads.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "05xg9ZYy9E90"
      }
    }
  ]
}