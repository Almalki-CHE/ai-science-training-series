{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "d670dc5edce54522bb627fc66b9427f6",
            "dd675e93b0c74304af3765f80de03c88",
            "75e82d2adb9d471487160d08f99b1c67",
            "f6bc1fa939c246639511ffde020352d0",
            "fe32b2badd6c43c3a5afb7eb80186f82",
            "cf71cd75557a4a06b2e2d0238c328198"
          ]
        },
        "id": "Otji65rzYI6c",
        "outputId": "4c476287-63f8-4b80-c7b8-1cfaec900ab7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d670dc5edce54522bb627fc66b9427f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train Epoch 0:   0%|          | 0/782 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd675e93b0c74304af3765f80de03c88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validate (train) Epoch 0:   0%|          | 0/782 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: training loss: 0.958, accuracy: 0.657\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e82d2adb9d471487160d08f99b1c67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validate Epoch 0:   0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: validation loss: 1.007, accuracy: 0.638\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6bc1fa939c246639511ffde020352d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe32b2badd6c43c3a5afb7eb80186f82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validate Epoch 1:   0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: validation loss: 0.796, accuracy: 0.722\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf71cd75557a4a06b2e2d0238c328198",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# CIFAR-10 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "train_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "val_data = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Modified CNN Model\n",
        "class ModifiedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModifiedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)  # Adjust input dimensions based on CIFAR-10 image size\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
        "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
        "        x = self.pool(nn.ReLU()(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = self.dropout(nn.ReLU()(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = ModifiedCNN()\n",
        "\n",
        "# Loss Function and Optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and Validation Functions\n",
        "def train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        images, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "def evaluate(dataloader, model, loss_fn, progress_bar):\n",
        "    model.eval()\n",
        "    total_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images, labels = batch\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            progress_bar.update(1)\n",
        "    return correct / len(dataloader.dataset), total_loss / len(dataloader)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 15\n",
        "training_losses, validation_losses = [], []\n",
        "training_accuracies, validation_accuracies = [], []\n",
        "\n",
        "for j in range(epochs):\n",
        "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n",
        "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n",
        "\n",
        "    if j % 5 == 0:\n",
        "        with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n",
        "            acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n",
        "            print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n",
        "            training_losses.append(loss)\n",
        "            training_accuracies.append(acc)\n",
        "\n",
        "    with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
        "        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n",
        "        print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")\n",
        "        validation_losses.append(loss_val)\n",
        "        validation_accuracies.append(acc_val)\n",
        "\n",
        "# Plotting Results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range[::5], training_losses, label=\"Training Loss\")\n",
        "plt.plot(epochs_range, validation_losses, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss Over Epochs\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range[::5], training_accuracies, label=\"Training Accuracy\")\n",
        "plt.plot(epochs_range, validation_accuracies, label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy Over Epochs\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}